{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IT_Project_NCF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWZA89eShbXd"
      },
      "source": [
        "# Neural Collaborative Filtering\n",
        "Building a **Neural Matrix Factorization** model from scratch in PyTorch on MovieLens-100k dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlWV9QEheB29"
      },
      "source": [
        "!pip install -q tensorboardX"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1E54ayu9RON"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J17ZmmLNcx-G"
      },
      "source": [
        "## Downloading Movielens-100k Ratings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAa3Vwhq6agf"
      },
      "source": [
        "DATA_URL = \"https://raw.githubusercontent.com/SudeshGowda/ml-100k-dataset/main/u.data\"\n",
        "MAIN_PATH = '/content/'\n",
        "DATA_PATH = MAIN_PATH + 'u.data'\n",
        "MODEL_PATH = MAIN_PATH + 'models/'\n",
        "MODEL_NEUMF = 'ml-100k_NeuMF'\n",
        "MODEL_GMF = 'ml-100k_GMF'\n",
        "MODEL_MLP = 'ml-100k_MLP'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zM4ltadpAygl",
        "outputId": "b854a94e-047c-4f0c-d952-3101b4cbacd7"
      },
      "source": [
        "!wget -nc https://raw.githubusercontent.com/SudeshGowda/ml-100k-dataset/main/u.data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File â€˜u.dataâ€™ already there; not retrieving.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VT89-ZSZ9pMl"
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJH1FfkIems4"
      },
      "source": [
        "## Defining Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "663oFevqek3a"
      },
      "source": [
        "class Rating_Datset(torch.utils.data.Dataset):\n",
        "\tdef __init__(self, user_list, item_list, rating_list):\n",
        "\t\tsuper(Rating_Datset, self).__init__()\n",
        "\t\tself.user_list = user_list\n",
        "\t\tself.item_list = item_list\n",
        "\t\tself.rating_list = rating_list\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.user_list)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tuser = self.user_list[idx]\n",
        "\t\titem = self.item_list[idx]\n",
        "\t\trating = self.rating_list[idx]\n",
        "\t\t\n",
        "\t\treturn (\n",
        "\t\t\ttorch.tensor(user, dtype=torch.long),\n",
        "\t\t\ttorch.tensor(item, dtype=torch.long),\n",
        "\t\t\ttorch.tensor(rating, dtype=torch.float)\n",
        "\t\t\t)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4xgxyBsfoJM"
      },
      "source": [
        "### NCF Dataset Class\n",
        "- *_reindex*: process dataset to reindex userID and itemID, also set rating as binary feedback\n",
        "- *_leave_one_out*: leave-one-out evaluation\n",
        "- *negative_sampling*: randomly selects n negative examples for each positive one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDXTuM0C6BGr"
      },
      "source": [
        "class NCF_Data(object):\n",
        "\t\"\"\"\n",
        "\tConstruct Dataset for NCF\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, args, ratings):\n",
        "\t\tself.ratings = ratings\n",
        "\t\tself.num_ng = args.num_ng\n",
        "\t\tself.num_ng_test = args.num_ng_test\n",
        "\t\tself.batch_size = args.batch_size\n",
        "\n",
        "\t\tself.preprocess_ratings = self._reindex(self.ratings)\n",
        "\n",
        "\t\tself.user_pool = set(self.ratings['user_id'].unique())\n",
        "\t\tself.item_pool = set(self.ratings['item_id'].unique())\n",
        "\n",
        "\t\tself.train_ratings, self.test_ratings = self._leave_one_out(self.preprocess_ratings)\n",
        "\t\tself.negatives = self._negative_sampling(self.preprocess_ratings)\n",
        "\t\trandom.seed(args.seed)\n",
        "\t\n",
        "\tdef _reindex(self, ratings):\n",
        "\t\t\"\"\"\n",
        "\t\tProcess dataset to reindex userID and itemID, also set rating as binary feedback\n",
        "\t\t\"\"\"\n",
        "\t\tuser_list = list(ratings['user_id'].drop_duplicates())\n",
        "\t\tuser2id = {w: i for i, w in enumerate(user_list)}\n",
        "\n",
        "\t\titem_list = list(ratings['item_id'].drop_duplicates())\n",
        "\t\titem2id = {w: i for i, w in enumerate(item_list)}\n",
        "\n",
        "\t\tratings['user_id'] = ratings['user_id'].apply(lambda x: user2id[x])\n",
        "\t\tratings['item_id'] = ratings['item_id'].apply(lambda x: item2id[x])\n",
        "\t\tratings['rating'] = ratings['rating'].apply(lambda x: float(x > 0))\n",
        "\t\treturn ratings\n",
        "\n",
        "\tdef _leave_one_out(self, ratings):\n",
        "\t\tratings['rank_latest'] = ratings.groupby(['user_id'])['timestamp'].rank(method='first', ascending=False)\n",
        "\t\ttest = ratings.loc[ratings['rank_latest'] == 1]\n",
        "\t\ttrain = ratings.loc[ratings['rank_latest'] > 1]\n",
        "\t\tassert train['user_id'].nunique()==test['user_id'].nunique(), 'Not Match Train User with Test User'\n",
        "\t\treturn train[['user_id', 'item_id', 'rating']], test[['user_id', 'item_id', 'rating']]\n",
        "\n",
        "\tdef _negative_sampling(self, ratings):\n",
        "\t\tinteract_status = (\n",
        "\t\t\tratings.groupby('user_id')['item_id']\n",
        "\t\t\t.apply(set)\n",
        "\t\t\t.reset_index()\n",
        "\t\t\t.rename(columns={'item_id': 'interacted_items'}))\n",
        "\t\tinteract_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
        "\t\tinteract_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, self.num_ng_test))\n",
        "\t\treturn interact_status[['user_id', 'negative_items', 'negative_samples']]\n",
        "\n",
        "\tdef get_train_instance(self):\n",
        "\t\tusers, items, ratings = [], [], []\n",
        "\t\ttrain_ratings = pd.merge(self.train_ratings, self.negatives[['user_id', 'negative_items']], on='user_id')\n",
        "\t\ttrain_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, self.num_ng))\n",
        "\t\tfor row in train_ratings.itertuples():\n",
        "\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\titems.append(int(row.item_id))\n",
        "\t\t\tratings.append(float(row.rating))\n",
        "\t\t\tfor i in range(self.num_ng):\n",
        "\t\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\t\titems.append(int(row.negatives[i]))\n",
        "\t\t\t\tratings.append(float(0))  # negative samples get 0 rating\n",
        "\t\tdataset = Rating_Datset(\n",
        "\t\t\tuser_list=users,\n",
        "\t\t\titem_list=items,\n",
        "\t\t\trating_list=ratings)\n",
        "\t\treturn torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "\tdef get_test_instance(self):\n",
        "\t\tusers, items, ratings = [], [], []\n",
        "\t\ttest_ratings = pd.merge(self.test_ratings, self.negatives[['user_id', 'negative_samples']], on='user_id')\n",
        "\t\tfor row in test_ratings.itertuples():\n",
        "\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\titems.append(int(row.item_id))\n",
        "\t\t\tratings.append(float(row.rating))\n",
        "\t\t\tfor i in getattr(row, 'negative_samples'):\n",
        "\t\t\t\tusers.append(int(row.user_id))\n",
        "\t\t\t\titems.append(int(i))\n",
        "\t\t\t\tratings.append(float(0))\n",
        "\t\tdataset = Rating_Datset(\n",
        "\t\t\tuser_list=users,\n",
        "\t\t\titem_list=items,\n",
        "\t\t\trating_list=ratings)\n",
        "\t\treturn torch.utils.data.DataLoader(dataset, batch_size=self.num_ng_test+1, shuffle=False, num_workers=2)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POQwMIk2dSki"
      },
      "source": [
        "## Defining Metrics\n",
        "Using Hit Rate and NDCG as our evaluation *metrics*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH6x2T559cL1"
      },
      "source": [
        "def hit(ng_item, pred_items):\n",
        "\tif ng_item in pred_items:\n",
        "\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def ndcg(ng_item, pred_items):\n",
        "\tif ng_item in pred_items:\n",
        "\t\tindex = pred_items.index(ng_item)\n",
        "\t\treturn np.log2(2)/np.log2(index+2)\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def metrics(model, test_loader, top_k, device):\n",
        "\tHR, NDCG = [], []\n",
        "\n",
        "\tfor user, item, label in test_loader:\n",
        "\t\tuser = user.to(device)\n",
        "\t\titem = item.to(device)\n",
        "\n",
        "\t\tpredictions = model(user, item)\n",
        "\t\t_, indices = torch.topk(predictions, top_k)\n",
        "\t\trecommends = torch.take(\n",
        "\t\t\t\titem, indices).cpu().numpy().tolist()\n",
        "\n",
        "\t\tng_item = item[0].item() # leave one-out evaluation has only one item per user\n",
        "\t\tHR.append(hit(ng_item, recommends))\n",
        "\t\tNDCG.append(ndcg(ng_item, recommends))\n",
        "\n",
        "\treturn np.mean(HR), np.mean(NDCG)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waDSZHz7dlNo"
      },
      "source": [
        "### Defining Model Architectures\n",
        "1. Generalized Matrix Factorization\n",
        "2. Multi Layer Perceptron\n",
        "3. Neural Matrix Factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTQaitu7d1R3"
      },
      "source": [
        "class Generalized_Matrix_Factorization(nn.Module):\n",
        "    def __init__(self, args, num_users, num_items):\n",
        "        super(Generalized_Matrix_Factorization, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.factor_num = args.factor_num\n",
        "\n",
        "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n",
        "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n",
        "\n",
        "        self.affine_output = nn.Linear(in_features=self.factor_num, out_features=1)\n",
        "        self.logistic = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        element_product = torch.mul(user_embedding, item_embedding)\n",
        "        logits = self.affine_output(element_product)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating.squeeze()\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kSFzPlNd50f"
      },
      "source": [
        "class Multi_Layer_Perceptron(nn.Module):\n",
        "    def __init__(self, args, num_users, num_items):\n",
        "        super(Multi_Layer_Perceptron, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.factor_num = args.factor_num\n",
        "        self.layers = args.layers\n",
        "\n",
        "        self.embedding_user = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num)\n",
        "        self.embedding_item = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num)\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):\n",
        "            self.fc_layers.append(nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = nn.Linear(in_features=self.layers[-1], out_features=1)\n",
        "        self.logistic = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            vector = self.fc_layers[idx](vector)\n",
        "            vector = nn.ReLU()(vector)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating.squeeze()\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DQpVuaV9cF0"
      },
      "source": [
        "class NeuMF(nn.Module):\n",
        "    def __init__(self, args, num_users, num_items):\n",
        "        super(NeuMF, self).__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.factor_num_mf = args.factor_num\n",
        "        self.factor_num_mlp =  int(args.layers[0]/2)\n",
        "        self.layers = args.layers\n",
        "\n",
        "        self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)\n",
        "        self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)\n",
        "\n",
        "        self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf)\n",
        "        self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf)\n",
        "\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "            self.fc_layers.append(nn.ReLU())\n",
        "\n",
        "        self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_mf, out_features=1)\n",
        "        self.logistic = nn.Sigmoid()\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        nn.init.normal_(self.embedding_user_mlp.weight, std=0.01)\n",
        "        nn.init.normal_(self.embedding_item_mlp.weight, std=0.01)\n",
        "        nn.init.normal_(self.embedding_user_mf.weight, std=0.01)\n",
        "        nn.init.normal_(self.embedding_item_mf.weight, std=0.01)\n",
        "        \n",
        "        for m in self.fc_layers:\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                \n",
        "        nn.init.xavier_uniform_(self.affine_output.weight)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
        "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
        "\n",
        "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
        "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
        "\n",
        "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)\n",
        "        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
        "\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
        "\n",
        "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating.squeeze()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpBX6rqNfSc9"
      },
      "source": [
        "### Setting Arguments\n",
        "\n",
        "Here is the brief description of important ones:\n",
        "- Learning rate is 0.001\n",
        "- HitRate@10 and NDCG@10\n",
        "- 4 negative samples for each positive one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc5Vg1Ik_gnF",
        "outputId": "fcb5c5bf-1749-48e5-d35e-bbb89c032e18"
      },
      "source": [
        "#collapse-hide\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--seed\", \n",
        "\ttype=int, \n",
        "\tdefault=42, \n",
        "\thelp=\"Seed\")\n",
        "parser.add_argument(\"--lr\", \n",
        "\ttype=float, \n",
        "\tdefault=0.001, \n",
        "\thelp=\"learning rate\")\n",
        "parser.add_argument(\"--batch_size\", \n",
        "\ttype=int, \n",
        "\tdefault=256, \n",
        "\thelp=\"batch size for training\")\n",
        "parser.add_argument(\"--neumf_epochs\", \n",
        "\ttype=int,\n",
        "\tdefault=10,  \n",
        "\thelp=\"training epoches for neumf\")\n",
        "parser.add_argument(\"--mlp_epochs\", \n",
        "\ttype=int,\n",
        "\tdefault=20,  \n",
        "\thelp=\"training epoches for neumf\")\n",
        "parser.add_argument(\"--gmf_epochs\", \n",
        "\ttype=int,\n",
        "\tdefault=20,  \n",
        "\thelp=\"training epoches for neumf\")\n",
        "parser.add_argument(\"--top_k\", \n",
        "\ttype=int, \n",
        "\tdefault=10, \n",
        "\thelp=\"compute metrics@top_k\")\n",
        "parser.add_argument(\"--factor_num\", \n",
        "\ttype=int,\n",
        "\tdefault=32, \n",
        "\thelp=\"predictive factors numbers in the model\")\n",
        "parser.add_argument(\"--layers\",\n",
        "    nargs='+', \n",
        "    default=[64,32,16,8],\n",
        "    help=\"MLP layers. Note that the first layer is the concatenation of user \\\n",
        "    and item embeddings. So layers[0]/2 is the embedding size.\")\n",
        "parser.add_argument(\"--num_ng\", \n",
        "\ttype=int,\n",
        "\tdefault=4, \n",
        "\thelp=\"Number of negative samples for training set\")\n",
        "parser.add_argument(\"--num_ng_test\", \n",
        "\ttype=int,\n",
        "\tdefault=100, \n",
        "\thelp=\"Number of negative samples for test set\")\n",
        "parser.add_argument(\"--out\", \n",
        "\tdefault=True,\n",
        "\thelp=\"save model or not\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_StoreAction(option_strings=['--out'], dest='out', nargs=None, const=None, default=True, type=None, choices=None, help='save model or not', metavar=None)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyWquJG893CV",
        "outputId": "8b9bcd3b-291e-4d64-d62f-02b2e3ba6a08"
      },
      "source": [
        "# set device and parameters\n",
        "args = parser.parse_args(\"\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# seed for Reproducibility\n",
        "seed_everything(args.seed)\n",
        "\n",
        "# load data\n",
        "ml_100k = pd.read_csv(\n",
        "\tDATA_PATH, \n",
        "\tsep=\"\\t\", \n",
        "\tnames = ['user_id', 'item_id', 'rating', 'timestamp'], \n",
        "\tengine='python')\n",
        "\n",
        "# set the num_users, items\n",
        "num_users = ml_100k['user_id'].nunique()+1\n",
        "num_items = ml_100k['item_id'].nunique()+1\n",
        "\n",
        "# construct the train and test datasets\n",
        "data = NCF_Data(args, ml_100k)\n",
        "train_loader = data.get_train_instance()\n",
        "test_loader = data.get_test_instance()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnaRWy2gg_Nw"
      },
      "source": [
        "## Training NeuMF Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9b5CERce8ed",
        "outputId": "d2e5db74-7131-4671-d7a3-54361881889e"
      },
      "source": [
        "writer = SummaryWriter()\n",
        "# set model and loss, optimizer\n",
        "model = NeuMF(args, num_users, num_items)\n",
        "model = model.to(device)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "# train, evaluation\n",
        "best_hr = 0\n",
        "for epoch in range(1, args.neumf_epochs+1):\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for user, item, label in train_loader:\n",
        "        user = user.to(device)\n",
        "        item = item.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(user, item)\n",
        "        loss = loss_function(prediction, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        writer.add_scalar('NeuMF/loss/Train_loss', loss.item(), epoch)\n",
        "\n",
        "    model.eval()\n",
        "    HR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
        "    writer.add_scalar('NeuMF/Perfomance/HR@10', HR, epoch)\n",
        "    writer.add_scalar('NeuMF/Perfomance/NDCG@10', NDCG, epoch)\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    print(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n",
        "            time.str!lsftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "    print(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "    if HR > best_hr:\n",
        "        best_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
        "        if args.out:\n",
        "            if not os.path.exists(MODEL_PATH):\n",
        "                os.mkdir(MODEL_PATH)\n",
        "            torch.save(model, \n",
        "                '{}{}.pth'.format(MODEL_PATH, MODEL_NEUMF))\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 001 is: 00: 00: 26\n",
            "HR: 0.549\tNDCG: 0.309\n",
            "The time elapse of epoch 002 is: 00: 00: 26\n",
            "HR: 0.602\tNDCG: 0.343\n",
            "The time elapse of epoch 003 is: 00: 00: 26\n",
            "HR: 0.630\tNDCG: 0.361\n",
            "The time elapse of epoch 004 is: 00: 00: 26\n",
            "HR: 0.645\tNDCG: 0.373\n",
            "The time elapse of epoch 005 is: 00: 00: 26\n",
            "HR: 0.657\tNDCG: 0.385\n",
            "The time elapse of epoch 006 is: 00: 00: 26\n",
            "HR: 0.657\tNDCG: 0.386\n",
            "The time elapse of epoch 007 is: 00: 00: 27\n",
            "HR: 0.653\tNDCG: 0.378\n",
            "The time elapse of epoch 008 is: 00: 00: 26\n",
            "HR: 0.644\tNDCG: 0.373\n",
            "The time elapse of epoch 009 is: 00: 00: 27\n",
            "HR: 0.633\tNDCG: 0.365\n",
            "The time elapse of epoch 010 is: 00: 00: 27\n",
            "HR: 0.617\tNDCG: 0.359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19QeHOmKlscm",
        "outputId": "9a00922b-ffae-4a85-8186-c7057c97ebbd"
      },
      "source": [
        "print(\"NeuMF best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format(best_epoch, best_hr, best_ndcg))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuMF best epoch 005: HR = 0.657, NDCG = 0.385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5LFJof0kNwf"
      },
      "source": [
        "## Training MLP model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgK2QXhzhUA1",
        "outputId": "faeab417-f521-45e4-c591-44d6df810970"
      },
      "source": [
        "writer = SummaryWriter()\n",
        "# set model and loss, optimizer\n",
        "model = Multi_Layer_Perceptron(args, num_users, num_items)\n",
        "model = model.to(device)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "# train, evaluation\n",
        "\n",
        "best_hr = 0\n",
        "for epoch in range(1, args.mlp_epochs+1):\n",
        "\tmodel.train()\n",
        "\tstart_time = time.time()\n",
        "\n",
        "\tfor user, item, label in train_loader:\n",
        "\t\tuser = user.to(device)\n",
        "\t\titem = item.to(device)\n",
        "\t\tlabel = label.to(device)\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tprediction = model(user, item)\n",
        "\t\tloss = loss_function(prediction, label)\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\t\twriter.add_scalar('MLP/loss/Train_loss', loss.item(), epoch)\n",
        "\n",
        "\tmodel.eval()\n",
        "\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
        "\twriter.add_scalar('MLP/Perfomance/HR@10', HR, epoch)\n",
        "\twriter.add_scalar('MLP/Perfomance/NDCG@10', NDCG, epoch)\n",
        "\n",
        "\telapsed_time = time.time() - start_time\n",
        "\tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n",
        "\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "\tif HR > best_hr:\n",
        "\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
        "\t\tif args.out:\n",
        "\t\t\tif not os.path.exists(MODEL_PATH):\n",
        "\t\t\t\tos.mkdir(MODEL_PATH)\n",
        "\t\t\ttorch.save(model, \n",
        "\t\t\t\t'{}{}.pth'.format(MODEL_PATH, MODEL_MLP))\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 001 is: 00: 00: 26\n",
            "HR: 0.392\tNDCG: 0.216\n",
            "The time elapse of epoch 002 is: 00: 00: 26\n",
            "HR: 0.393\tNDCG: 0.222\n",
            "The time elapse of epoch 003 is: 00: 00: 26\n",
            "HR: 0.386\tNDCG: 0.217\n",
            "The time elapse of epoch 004 is: 00: 00: 25\n",
            "HR: 0.400\tNDCG: 0.219\n",
            "The time elapse of epoch 005 is: 00: 00: 25\n",
            "HR: 0.404\tNDCG: 0.218\n",
            "The time elapse of epoch 006 is: 00: 00: 25\n",
            "HR: 0.422\tNDCG: 0.231\n",
            "The time elapse of epoch 007 is: 00: 00: 25\n",
            "HR: 0.418\tNDCG: 0.233\n",
            "The time elapse of epoch 008 is: 00: 00: 25\n",
            "HR: 0.422\tNDCG: 0.239\n",
            "The time elapse of epoch 009 is: 00: 00: 25\n",
            "HR: 0.442\tNDCG: 0.249\n",
            "The time elapse of epoch 010 is: 00: 00: 26\n",
            "HR: 0.460\tNDCG: 0.259\n",
            "The time elapse of epoch 011 is: 00: 00: 25\n",
            "HR: 0.467\tNDCG: 0.265\n",
            "The time elapse of epoch 012 is: 00: 00: 26\n",
            "HR: 0.473\tNDCG: 0.274\n",
            "The time elapse of epoch 013 is: 00: 00: 26\n",
            "HR: 0.495\tNDCG: 0.282\n",
            "The time elapse of epoch 014 is: 00: 00: 26\n",
            "HR: 0.508\tNDCG: 0.288\n",
            "The time elapse of epoch 015 is: 00: 00: 25\n",
            "HR: 0.512\tNDCG: 0.288\n",
            "The time elapse of epoch 016 is: 00: 00: 25\n",
            "HR: 0.515\tNDCG: 0.291\n",
            "The time elapse of epoch 017 is: 00: 00: 26\n",
            "HR: 0.516\tNDCG: 0.294\n",
            "The time elapse of epoch 018 is: 00: 00: 25\n",
            "HR: 0.528\tNDCG: 0.298\n",
            "The time elapse of epoch 019 is: 00: 00: 26\n",
            "HR: 0.537\tNDCG: 0.299\n",
            "The time elapse of epoch 020 is: 00: 00: 25\n",
            "HR: 0.546\tNDCG: 0.302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sicxhWEelra2",
        "outputId": "7faf5430-1253-4693-9de1-07e211f4fe5e"
      },
      "source": [
        "print(\"MLP best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format(best_epoch, best_hr, best_ndcg))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP best epoch 020: HR = 0.546, NDCG = 0.302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaTb9OMtkXPw"
      },
      "source": [
        "## Training GMF model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-RHuFgMhVEz",
        "outputId": "f2eb0050-ff08-4dc2-c7ce-d91e28a0510b"
      },
      "source": [
        "writer = SummaryWriter()\n",
        "# set model and loss, optimizer\n",
        "model = Generalized_Matrix_Factorization(args, num_users, num_items)\n",
        "model = model.to(device)\n",
        "loss_function = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "# train, evaluation\n",
        "best_hr = 0\n",
        "for epoch in range(1, args.gmf_epochs+1):\n",
        "\tmodel.train()\n",
        "\tstart_time = time.time()\n",
        "\n",
        "\tfor user, item, label in train_loader:\n",
        "\t\tuser = user.to(device)\n",
        "\t\titem = item.to(device)\n",
        "\t\tlabel = label.to(device)\n",
        "\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\tprediction = model(user, item)\n",
        "\t\tloss = loss_function(prediction, label)\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\t\twriter.add_scalar('GMF/loss/Train_loss', loss.item(), epoch)\n",
        "\n",
        "\tmodel.eval()\n",
        "\tHR, NDCG = metrics(model, test_loader, args.top_k, device)\n",
        "\twriter.add_scalar('GMF/Perfomance/HR@10', HR, epoch)\n",
        "\twriter.add_scalar('GMF/Perfomance/NDCG@10', NDCG, epoch)\n",
        "\n",
        "\telapsed_time = time.time() - start_time\n",
        "\tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n",
        "\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "\tif HR > best_hr:\n",
        "\t\tbest_hr, best_ndcg, best_epoch = HR, NDCG, epoch\n",
        "\t\tif args.out:\n",
        "\t\t\tif not os.path.exists(MODEL_PATH):\n",
        "\t\t\t\tos.mkdir(MODEL_PATH)\n",
        "\t\t\ttorch.save(model, \n",
        "\t\t\t\t'{}{}.pth'.format(MODEL_PATH, MODEL_GMF))\n",
        "\n",
        "writer.close()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 001 is: 00: 00: 22\n",
            "HR: 0.092\tNDCG: 0.043\n",
            "The time elapse of epoch 002 is: 00: 00: 22\n",
            "HR: 0.090\tNDCG: 0.042\n",
            "The time elapse of epoch 003 is: 00: 00: 21\n",
            "HR: 0.104\tNDCG: 0.047\n",
            "The time elapse of epoch 004 is: 00: 00: 22\n",
            "HR: 0.136\tNDCG: 0.063\n",
            "The time elapse of epoch 005 is: 00: 00: 22\n",
            "HR: 0.190\tNDCG: 0.091\n",
            "The time elapse of epoch 006 is: 00: 00: 22\n",
            "HR: 0.256\tNDCG: 0.124\n",
            "The time elapse of epoch 007 is: 00: 00: 22\n",
            "HR: 0.310\tNDCG: 0.155\n",
            "The time elapse of epoch 008 is: 00: 00: 22\n",
            "HR: 0.326\tNDCG: 0.170\n",
            "The time elapse of epoch 009 is: 00: 00: 22\n",
            "HR: 0.349\tNDCG: 0.188\n",
            "The time elapse of epoch 010 is: 00: 00: 22\n",
            "HR: 0.380\tNDCG: 0.205\n",
            "The time elapse of epoch 011 is: 00: 00: 22\n",
            "HR: 0.403\tNDCG: 0.216\n",
            "The time elapse of epoch 012 is: 00: 00: 22\n",
            "HR: 0.437\tNDCG: 0.231\n",
            "The time elapse of epoch 013 is: 00: 00: 21\n",
            "HR: 0.453\tNDCG: 0.241\n",
            "The time elapse of epoch 014 is: 00: 00: 22\n",
            "HR: 0.481\tNDCG: 0.256\n",
            "The time elapse of epoch 015 is: 00: 00: 22\n",
            "HR: 0.495\tNDCG: 0.263\n",
            "The time elapse of epoch 016 is: 00: 00: 22\n",
            "HR: 0.504\tNDCG: 0.269\n",
            "The time elapse of epoch 017 is: 00: 00: 22\n",
            "HR: 0.508\tNDCG: 0.273\n",
            "The time elapse of epoch 018 is: 00: 00: 22\n",
            "HR: 0.521\tNDCG: 0.278\n",
            "The time elapse of epoch 019 is: 00: 00: 22\n",
            "HR: 0.529\tNDCG: 0.282\n",
            "The time elapse of epoch 020 is: 00: 00: 22\n",
            "HR: 0.538\tNDCG: 0.287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkiRJWeD_trR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55e5d82d-fa35-4d99-86f2-f5753609f630"
      },
      "source": [
        "print(\"GMF best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format(best_epoch, best_hr, best_ndcg))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GMF best epoch 020: HR = 0.538, NDCG = 0.287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q3zusqge4EV"
      },
      "source": [
        "## Pushing SummaryWriter data to tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z9Fm6gjfl5Y",
        "outputId": "d73c22a8-eccd-46ae-bdc9-23c6ba71d3f2"
      },
      "source": [
        "# !tensorboard dev upload --logdir /content/runs/ --name \"Neural Collaborative filtering final\" --description \"Comparision of collaborative filtering of models\" --one_shot"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/7d8H8rLKTzKBR92X5wnwDA/\n",
            "\n",
            "\u001b[1m[2021-11-25T12:35:02]\u001b[0m Started scanning logdir.\n",
            "\u001b[1m[2021-11-25T12:35:35]\u001b[0m Total uploaded: 96850 scalars, 0 tensors, 0 binary objects\n",
            "\u001b[1m[2021-11-25T12:35:35]\u001b[0m Done scanning logdir.\n",
            "\n",
            "\n",
            "Done. View your TensorBoard at https://tensorboard.dev/experiment/7d8H8rLKTzKBR92X5wnwDA/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAWOH_OEfHeV"
      },
      "source": [
        "## Downloading model files and SummaryWriter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "x12ja8HifzXH",
        "outputId": "75d61fb7-79b0-411a-a0e0-7b529ce9dcc5"
      },
      "source": [
        "'''\n",
        "!zip -r /content/file.zip /content/models /content/runs\n",
        "from google.colab import files\n",
        "files.download('/content/file.zip') \n",
        "'''"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/ml-100k_NeuMF.pth (deflated 8%)\n",
            "  adding: content/models/ml-100k_GMF.pth (deflated 8%)\n",
            "  adding: content/models/ml-100k_MLP.pth (deflated 8%)\n",
            "  adding: content/runs/ (stored 0%)\n",
            "  adding: content/runs/Nov25_12-13-06_a33fc4fe255d/ (stored 0%)\n",
            "  adding: content/runs/Nov25_12-13-06_a33fc4fe255d/events.out.tfevents.1637842386.a33fc4fe255d (deflated 77%)\n",
            "  adding: content/runs/Nov25_12-18-03_a33fc4fe255d/ (stored 0%)\n",
            "  adding: content/runs/Nov25_12-18-03_a33fc4fe255d/events.out.tfevents.1637842683.a33fc4fe255d (deflated 76%)\n",
            "  adding: content/runs/Nov25_12-26-57_a33fc4fe255d/ (stored 0%)\n",
            "  adding: content/runs/Nov25_12-26-57_a33fc4fe255d/events.out.tfevents.1637843217.a33fc4fe255d (deflated 76%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9b96306c-da1c-472f-be20-41848b2ebb66\", \"file.zip\", 2596511)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}